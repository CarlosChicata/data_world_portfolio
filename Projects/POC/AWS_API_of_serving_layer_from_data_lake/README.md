# Case 1: API of serving layer stored from data lake in AWS 

## Purpose and warning

I have a data model ready to be consumed for several users and apps. This model is stored in data lake by data architect decision. So you have a specified queries that are consumed the most of users in your case; so then i wanna implement a API to give access this users.

My acceptance criterias are:

* Management of access in specified endpoints in API by users. :boom: __Note__ : Because the complexity to solve in this POC, i will it in version 2.
* Management a validation credentials to autenticate the usage of API. :heavy_check_mark:
* Management of access of data in data model associated this specified client. :heavy_check_mark:
* Mecanism to read data based of data model from data lake. :heavy_check_mark:
* Mecanism to apply anonimization in sensitive data of data model. (optional) :boom: __Note__ : Because the complexity to solve in this POC, i will it in version 2.

## Note about the problem context

There are some point you need to know to understand better the problem:

First, this is the data model to extract data. This is a galaxy schema from dimensional data modelling. The theme of the serving data layer is about logistic delivery of product. **Note**: don't focus in design of data model, i don't focus to be correct design to check if the model effects someway the POC case.

![Data model](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWS_API_of_serving_layer_from_data_lake/code/image/POC%20serving%20layer%20-%20data%20model.png)

Second, this is the list of common queries will be consumed by our clients:

1. Get all detailed trackcode based in datetime range. I say `all_orders_by_range`.
2. Get all money generated by the routes and detailed in range of datetime. I say `get_money_from_routes_by_range`.
3. Get the most visited location from the trackcodes in range of datetime. I say `most_visited_location_from_trackcode_by_range`.
4. The number of trackcode generated based date in range of datetime. I say `count_trackcode_by_range`.
5. The number of orders by code generated during a date range. I say `count_orders_by_range`.
6. The number of trackcodes lost. I say `count_trackcode_lost_by_range`.
7. The most required service using during a date range. I say `most_required_service_by_range`.
8. Check how many orders are success delivery before the promise time in date range. I say `count_delivered_trackcode_before_promise_time_by_range`.


## Challenges

These are the main challenges to face:

1. We need to check if the user's token is authorized to access the request's endpoint :heavy_check_mark:
2. We need to check if the user's token is validated to use. :heavy_check_mark:
3. Make a Custom validations in AWS gateway for 1-2 points. :heavy_check_mark:
4. Make a custom queries parameters in AWS Athena based in specified role. :boom: __Note__ : For this POC i've chosen don't implement this part by complexity to solve. In version 2 this POC i will it.

## Solution

### General Idea

I will implement a custom authorization process to check the authorization and authentication of request from API, then processing the request with specified sql query and return it on JSON format.

![Infraestructure POC case 1](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWS_API_of_serving_layer_from_data_lake/code/image/diagrama%20poc%20case%201%20-%20personal.drawio.png)

### Tools to implement

1. AWS Athena
2. Python 3.9
3. AWS Lambda function and layers.
4. AWS S3 standard
5. AWS Cloudformation
6. AWS Glue: Database, crawler and job
7. AWS API Gateway: HTTP API

### Project Structure

 In the __Code__ folder contains all files associated this POC. This folder is structured following topics:
 
 * __auth_lambda_script__ folder contains all auth lambdas scripts associated to sql endpoints to extract data based in user request.
 * __fake_data__ folder contains all generated faking  data in CSV format used in POC.
 * __fake_parquet__ folder contains all generated faking  data in Parquet format used in POC.
 * __process_script__ folder contains python scripts to generate tables based in faked data and prepare environment in processing resources.
 * __sql_script__ folder contains pythons scripts associated the endpoints in API service to extract data based in user request.
 * __infraestructure_cloudformation.yaml__ file is AWS cloudformation based IaC to generate all infrastructure in POC.
 * __requirements.txt__ file is generate a virtual env. to developer in local enviroment.
 * __reference.txt__ and __demo_template_IAC_1.yaml__ files are documents to guide in this implementation POC.

### How to prepare this project?

:warning: __I did this part; but if you wanna know how i generate this tables and you wanna generate these in new format, Read this part.__

I have in __fake_data__ folder, several files with faked data i generated based in my experience. Although I used some fields, there are fields that I don't use; if you wanna experiment with this, do it! :smile:

If you wanna prepare the tables with data from CSV format to parquet format; you need to run 2 scripts: `main.py` y `load_tables.py` in __process_script__ folder. I created `load_tables.py` script to generate only access control table; and rest of tables i used `main.py` script. Yyou can run those in any order to create the tables.

### How to set up this project?

#### By video

Soon i will upload the videos in spanish and english.

#### By step-by-step Documentation 

First; you must uploaded inside S3 bucket you chosen, all python scripts associated to custom auth lambda files and SQL endpoints; the SQL endpoints are code to query on the tables using AWS Athena; because if you wanna use the AWS Cloudformation, you must stores those files in S3 to work. This bucket need to be created manually. Remember; custom auth lamabda and SQL endpoints are implemented with AWS Lambda function and available from AWS API Gateway to access it.

Second; In other or same S3 bucket  you chosen, stored all generated tables will use by AWS Athena; because the same reason in previous paragraph. This bucket need to be created manually. I used Job from AWS Glue to prepare the tables inside from S3 and Crawler from AWS Glue to generate tables of DB for AWS Athena.

Third; In AWS Cloudformation, i passed the `infraestructure_cloudformation.yaml` file to generate automatically all recourses in POC. I generate:

* AWS API Gateway, AWS Lambda function and AWS IAM (for lambda) for API to handle user requests.
* AWS Glue Job, data catalog and crawler services with AWS S3 to preparing the enviroment of AWS Athena to query tables.
* AWS Athena workgroup to store the result of queries.

![Generated resources from IaC template file ](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWS_API_of_serving_layer_from_data_lake/code/image/img-poc-case-1.png)

Fourth; Go AWS Glue Job, and execute the Job named _S3ToS3Moving_; this move all files in your S3 bucket in second paragraph into destination S3 bucket will store all data tables for AWS AThena.

![Executed AWS Glue Job](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWS_API_of_serving_layer_from_data_lake/code/image/img-poc-case-2.png)

Fifth; Go AWS Glue Crawler, and execute the all crawlers; they only are _poc-case-1-crawler_ and _poc-case-1-crawler-access-control-data_ ; to generate all tables need; those are data and access control tables respectivally. The AWS Cloudformation template had generated database resource of AWS Glue, so you don't bother about it.

![Executed AWS Glue crawlers](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWS_API_of_serving_layer_from_data_lake/code/image/img-poc-case-3.png)

Sixth; Check if all resources of data catalog are created and available to use in AWS Athena; so then it works without problems.

![Checked available AWS Athena](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWS_API_of_serving_layer_from_data_lake/code/image/img-poc-case-4.png)

Seventh; Check if all resources associated to AWS API Gateway are created and available to handle all user requests. 

![API Gateway worked](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWS_API_of_serving_layer_from_data_lake/code/image/img-poc-case-6.png)

Remember This API accept JSON request, with body with params following specifications:

| _Name of params_ | _Must be_ | _Description_ | _Structure_ |
|------------------|-----------|---------------|-------------|
| start_datetime | Yes | Start date to find  the resources | `YYYY-MM-DD` |
| end_datetime | Yes | End date to find the resources | `YYYY-MM-DD` |

### How destroy the POC project? (By step-by-step Documentation )

First; delete all object inside S3 buckets you created with AWS cloudformation template. Remember the non-automaticated created S3 buckets, you need to delete those manually, as AWS cloudformation template won't delete those for you.

![Delete all object in S3 buckets](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWS_API_of_serving_layer_from_data_lake/code/image/img-poc-case-7.png)

Second; Go the AWS Cloudformation to delete the stack of resources in the POC automatically. Be patient. :wink:

![Delete resource from AWS Cloudformation](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWST_API_of_serving_layer_from_data_lake/code/image/img-poc-case-8.png)

Third; check if all resources are deleted by AWS Cloudformation; if there any lived, delete those manually.

![Check if all resources are deleted](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWS_API_of_serving_layer_from_data_lake/code/image/img-poc-case-9.png)

### Topic issues

I needed to learn in my few free time some topics about AWS resource like Athena, Lambda, Glue, Cloudformation and API Gatewy; so i built based on simplicity and functional vision such that i got a POC in version 1 with minimum functionality to the main goal: __*delivery data on demand into my users.*__

#### Scalability: :star2::star2::star2::star2::star2:

:thumbsup: Because the architecture is serverless focus in general; the resources can created based in demand required in all kind of enviroment to solve all user requests. Any change in data or code can be apply in scale in this infrastructure model.


#### Performance: :star2::star2::star::star::star:

:thumbsup: Because the architecture is serverless focus in general; the HTTP API can responde in high request on 24/7. The auth lambda can cache the same authentication operation for same user.

:eyes: The auth lambda have a delay based in the performance of aws athena to get the data of access control table. I can understand the performance in SQL endpoint is accepted for now; but in high performance the time (average 10ms) can be a problem; so i need to optimize; in priority order; following issues: 

1. The data to use in AWS Athena (depend of use case)
2. The auth lambda response.
 
Based in AWS Athena behavier, i can do several same SQL operation with same params and don't use the previous generated data file.

Don't work for high amount of data need to return into the user because the transmission of data from API; and ; other point; depending of your format data in data can be update one easier.


#### Reusability: :star2::star2::star2::star::star:

:thumbsup: The great point is the IaC template to implement easily all resource AWS in the POC, so then i need to do minimum steps to implement total infrastructure of project. 

:eyes: One of the great problem in this POC is the duplicate of code in auth lambdas and sql endpoints: I consider the duplicate code in several auth lambdas and process like a chance to improve the structure of code and data model such that:

* In one query, i can get as amount as i can.
* Don't maintain several file with exactly like content for same goal.
* Get minimum and necessary logical processes in project.

#### Security :star2::star2::star::star::star:

:thumbsup: There are an authentication and authorization control the can be easy to use and apply inside the organization for the HTTP API.

:eyes: The IAM Roles need to be more restricted: anyone can be anything with these roles!. And all internal request in POC is running over internet, it expose to be hacked!.
