# Proof of concept from cloud! 

## Purpose and warning

The goal is show all proof of concept that i implemented based in several cloud providers. The main goal is show my hard skills to using the tools of several cloud providers to achieve a specified given goal by the POC. i want a be a good data engineering

## Type of project

This is the classification of projects that a POC can be oriented.

| Kind of project | the need | The goal |
|-----------------|----------|---------|
| Analytics | needs data to inform decision making.| Deliver insights |
| Automation | save time and/or money by replacing manual tasks with automated processes | Save money and be productive |
| Product development | Turn raw data into products from which revenue  can be generated | Make money |

## List of portfolio cases

This is my portfolio projects i built it based in this case.

| N¬∞ of Case | Title | Type of project| Short description | Tools | Cloud Provider | Concepts | Status | Name of project |
|------------|-------|----------------|-------------------|-------|----------------|----------|--------|-----------------|
| 1 | API of Serving layer from data lake | Product development  | Create a HTTP API to consume a data model is stored in data lake. | `AWS S3`, `AWS Athena`, `AWS API gateway`, `Python` | AWS | `Data lake`, `Data modeling`, `AWS tools`  | `Finished` :rocket: | AWS_API_of_serving_layer_from_data_lake |
| 2 | GraphQL API of Serving layer from data lake | Automation  | Create a GraphQL API to consume a data model is stored in data lake. | `AWS S3`, `AWS Athena`, `AWS AppSync`, `Python` | AWS |  `Data lake`, `Data modeling`, `AWS tools` | `Finished` ü§© | AWS_GRAPHQL_of_serving_layer_from_data_lake |
| 3 | API of Serving layer from data lake V2 | Product development | Optimize some point in case #1 to improve the performance and security |  `AWS IAM`,`AWS Athena`, `AWS API gateway`, `Python` | AWS | `Data lake`, `Data modeling`, `AWS tools`  | `In progress...`üôâ | `Soon` |
| 4 | Formatting for AWS Glue: CSV to delta lake format file | Automation | Search to way to format CSV file to delta lake file, and can be use for AWS Athena | `AWS Athena`, `Python`, `Dataframe framework`| AWS | `File formatting`, `AWS tools`, `Data conversion` | `Get ready to start` |  `Soon` |
| 5 | Formatting for AWS Glue: CSV to Iceberg format file | Automation | Search to way to format CSV file to Iceberg file, and can be use for AWS Athena | `AWS Athena`, `Python`, `Dataframe framework`| AWS |`File formatting`, `AWS tools`, `Data conversion` | `Finished` üí´  | AWS_convert_csv_to_iceberg_format |
| 6 | GraphQL API of Serving layer from data lake V2 | Automation | Optimize some point in case #2 to improve the performance |  `AWS IAM`,`AWS Athena`, `AWS AppSync`, `Python` | AWS | `Data lake`, `Data modeling`, `AWS tools`  | `Get ready to start` | `Soon` |
| 7 | Ingesting disaster tweets in real time | Automation | Processing tweets in (near) real time and store in data model of data warehouse. | `Python`, `AWS Kinesis Data Firehose`, `AWS Redshift`, `Twitter data`, `AWS Comprehend` | AWS + IA | `Real time processing`, ` Data ingestion`, `Analisis of Text`, `Classification AI` | `Finished` üòç | AWS_Ingesting_disaster_tweets_in_real_time |
| 8 | Voice-to-Data generator | Automation | Declaring What data need to IA and return a file with generated data from one data source. | `AWS S3`, `AWS SES`, `AWS RDS`, `AWS lambda`, `AI` | AWS + IA | `Data modeling`, `Integration system`, `Semantic layers`, `SQL IA generator` |  `Get ready to start` |  `Soon` |
| 9 | Search engine about video librery | Automation | Create a search engine to get videos in specified interval based in specified labels or images |  `AWS S3`, `AI` | AWS + IA | `Data lake`, `Ingestion process`, `Embedding vectors`, `Computational vision` |  `Get ready to start` |  `Soon` |
| 10 | Ingesting order events using data contracts | Automation | Create a data contract based pipeline to manage the data ingestion into raw layer. | `AWS SQS`, `AWS Kinesis stream`, `√ÄWS Lambda`| AWS | 3/5 | `Data contract`, `Data pipeline`, `Streaming processing`, `Data ingestation`, `Logistic` |  `Get ready to start` | `Soon` |
| 11 | Data pipeline implement: glue workflow or step-function | Automation | Comparing the implementation of data pipeline using glue workflow or  step-funtion | `AWS Glue`, `AWS Step function`, `AWS S3`, `AWS RDS`, `AWS Redshift` | AWS | 4/5 | `Data ingestation`, `Data pipeline`, `Processing Tools` | `Get ready to start` | `Soon` |
