# Case 1: REST API of serving layer stored from data lake in AWS 

## Purpose and warning

I have a data model ready to be consumed for several users and apps. This model is stored in data lake by data architect decision. So you have a specified queries that are consumed the most of users in your case; so then i wanna implement a REST API to give access this users.

My acceptance criterias are:

* Management of access in specified endpoints in API by users. :boom: __Note__ : Because the complexity to solve in this POC, i will it in version 2.
* Management a validation credentials to autenticate the usage of API. :heavy_check_mark:
* Management of access of data in data model associated this specified client. :heavy_check_mark:
* Mecanism to read data based of data model from data lake. :heavy_check_mark:
* Mecanism to apply anonimization in sensitive data of data model. (optional) :boom: __Note__ : Because the complexity to solve in this POC, i will it in version 2.

## Note about the problem context

There are some point you need to know to understand better the problem:

First, this is the data model to extract data. This is a galaxy schema from dimensional data modelling. The theme of the serving data layer is about logistic delivery of product. **Note**: don't focus in design of data model, i don't focus to be correct design to check if the model effects someway the POC case.

![Data model](https://github.com/CarlosChicata/data_world_portfolio/blob/main/Projects/POC/AWS_REST_API_of_serving_layer_from_data_lake/POC%20serving%20layer%20-%20data%20model.png)

Second, this is the list of common queries will be consumed by our clients:

1. Get all detailed trackcode based in datetime range. I say `all_orders_by_range`.
2. Get all money generated by the routes and detailed in range of datetime. I say `get_money_from_routes_by_range`.
3. Get the most visited location from the trackcodes in range of datetime. I say `most_visited_location_from_trackcode_by_range`.
4. The number of trackcode generated based date in range of datetime. I say `count_trackcode_by_range`.
5. The number of orders by code generated during a date range. I say `count_orders_by_range`.
6. The number of trackcodes lost. I say `count_trackcode_lost_by_range`.
7. The most required service using during a date range. I say `most_required_service_by_range`.
8. Check how many orders are success delivery before the promise time in date range. I say `count_delivered_trackcode_before_promise_time_by_range`.


## Challenges

These are the main challenges to face:

1. We need to check if the user's token is authorized to access the request's endpoint :heavy_check_mark:
2. We need to check if the user's token is validated to use. :heavy_check_mark:
3. Make a Custom validations in AWS gateway for 1-2 points. :heavy_check_mark:
4. Make a custom queries parameters in AWS Athena based in specified role. :boom: __Note__ : For this POC i've chosen don't implement this part by complexity to solve. In version 2 this POC i will it.

## Solution

### General Idea

I will implement a custom authorization process to check the authorization and authentication of request from API, then processing the request with specified sql query and return it on JSON format.

![Infraestructure POC case 1](https://github.com/CarlosChicata/data_world_portfolio/blob/master/Projects/POC/AWS_REST_API_of_serving_layer_from_data_lake/code/image/diagrama%20poc%20case%201%20-%20personal.drawio.png)

### Tools to implement

1. AWS Athena
2. Python 3.9
3. AWS Lambda function and layers.
4. AWS S3 standard
5. AWS Cloudformation
6. AWS Glue: Database, crawler and job
7. AWS API Gateway: REST API

### Project Structure

 In the __Code__ folder contains all files associated this POC. This folder is structured following topics:
 
 * __auth_lambda_script__ folder contains all auth lambdas scripts associated to sql endpoints to extract data based in user request.
 * __fake_data__ folder contains all generated faking  data in CSV format used in POC.
 * __fake_parquet__ folder contains all generated faking  data in Parquet format used in POC.
 * __process_script__ folder contains python scripts to generate tables based in faked data and prepare environment in processing resources.
 * __sql_script__ folder contains pythons scripts associated the endpoints in API service to extract data based in user request.
 * __infraestructure_cloudformation.yaml__ file is AWS cloudformation based IaC to generate all infrastructure in POC.
 * __requirements.txt__ file is generate a virtual env. to developer in local enviroment.
 * __reference.txt__ and __demo_template_IAC_1.yaml__ files are documents to guide in this implementation POC.

### How to implement this project?

### Topic issues

I needed to learn in my few free time some topics about AWS resource like Athena, Lambda, Glue, Cloudformation and API Gatewy; so i built based on simplicity and functional vision such that i got a POC in version 1 with minimum functionality to the main goal: __*delivery data on demand into my users.*__

#### Scalability: :star2::star2::star2::star2::star:

:thumbsup: 

#### Performance: :star2::star2::star::star::star:

:thumbsup: Because the architecture is serverless focus; the resources can created based in demand required in all kind of enviroment.

:eyes: The auth lambda have a delay based in the performance of aws athena to get the data of access control table. I can understand the performance in SQL endpoint is accepted for now; but in high performance the time (average 10ms) can be a problem; so i need to optimize; in priority order; following issues: 

1. The data to use in AWS Athena (depend of use case)
2. The auth lambda response.


#### Reusability code: :star2::star2::star2::star::star:

:thumbsup: The great point is the IaC template to implement easily all resource AWS in the POC, so then i need to do minimum steps to implement total infrastructure of project. 

:eyes: One of the great problem in this POC is the duplicate of code in auth lambdas and sql endpoints: I consider the duplicate code in several auth lambdas and process like a chance to improve the structure of code and data model such that:

* In one query, i can get as amount as i can.
* Don't maintain several file with exactly like content for same goal.
* Get minimum and necessary logical processes in project.

#### Security


#### Money

Money, design of solution, performance, scalability, possible problems and limitations of solution.


